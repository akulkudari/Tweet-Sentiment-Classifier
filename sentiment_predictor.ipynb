{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60004dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\akulk\\miniconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\akulk\\miniconda3\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\akulk\\miniconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\akulk\\miniconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: click in c:\\users\\akulk\\miniconda3\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\akulk\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e8224eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akulk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69533ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['sentiment', 'id', 'date', 'query', 'user', 'tweet']\n",
    "\n",
    "# list type to track total counts of negative (index 0) and positive (index 1) words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ffe82b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentData = pd.read_csv('training.1600000.processed.noemoticon.csv', delimiter=',', encoding='latin-1', names=column_names,\n",
    "    usecols=['sentiment', 'id', 'tweet'])\n",
    "# turn sentiment into a value between -1 and 1.\n",
    "sentimentData['sentiment'] = (sentimentData['sentiment'].astype(int) - 2) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d9affc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean through tweets and build a frequency list of the most common words in tweets, as well as their sentiment value aggregates\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "negation_words = {\"not\", \"no\", \"never\", \"n't\", \"cannot\", \"don't\", \"didn't\", \"doesn't\", \"won't\", \"can't\", \"shouldn't\"}\n",
    "\n",
    "def process_sentiment_data_prev(sentimentData):\n",
    "    \"\"\"\n",
    "    Processes tweet sentiment data to build word sentiment statistics. (NO negation filtering, or stopword filtering, or least frequent removal)\n",
    "    \n",
    "    Parameters:\n",
    "        sentimentData (pd.DataFrame): DataFrame with columns 'sentiment' and 'tweet'\n",
    "    \n",
    "    Returns:\n",
    "        word_stats (defaultdict): Dictionary mapping word → [neg_count, pos_count, sentiment_sum]\n",
    "        counts (list): [total_neg_tokens, total_pos_tokens, total_tokens]\n",
    "    \"\"\"\n",
    "    word_stats = defaultdict(lambda: [0, 0, 0])  # [neg_count, pos_count, sentiment_sum]\n",
    "    counts = [0, 0, 0]  # total negative, total positive, total tokens\n",
    "    \n",
    "    pattern = re.compile(r\"\\w+|[^\\w\\s]\")  # match words or punctuation\n",
    "\n",
    "    for row in sentimentData.itertuples(index=False):\n",
    "        sentiment = int(row.sentiment)\n",
    "        sentiment_index = (sentiment + 1) // 2\n",
    "        tweet = row.tweet.lower()\n",
    "        tokens = pattern.findall(tweet)\n",
    "\n",
    "        # Local counters to reduce write contention\n",
    "        local_neg = 0\n",
    "        local_pos = 0\n",
    "        local_total = 0\n",
    "\n",
    "        for token in tokens:\n",
    "            stats = word_stats[token]\n",
    "            stats[sentiment_index] += 1\n",
    "            stats[2] += sentiment\n",
    "\n",
    "            if sentiment_index == 0:\n",
    "                local_neg += 1\n",
    "            else:\n",
    "                local_pos += 1\n",
    "            local_total += 1\n",
    "\n",
    "        counts[0] += local_neg\n",
    "        counts[1] += local_pos\n",
    "        counts[2] += local_total\n",
    "\n",
    "    return word_stats, counts\n",
    "\n",
    "def process_sentiment_data_1(sentimentData):\n",
    "    \"\"\"\n",
    "    Processes tweet sentiment data to build word sentiment statistics with negation handling.\n",
    "    \n",
    "    Parameters:\n",
    "        sentimentData (pd.DataFrame): DataFrame with columns 'sentiment' and 'tweet'\n",
    "    \n",
    "    Returns:\n",
    "        word_stats (defaultdict): Dictionary mapping word → [neg_count, pos_count, sentiment_sum]\n",
    "        counts (list): [total_neg_tokens, total_pos_tokens, total_tokens]\n",
    "    \"\"\"\n",
    "    word_stats = defaultdict(lambda: [0, 0, 0])  # [neg_count, pos_count, sentiment_sum]\n",
    "    counts = [0, 0, 0]  # total negative, total positive, total tokens\n",
    "    \n",
    "    pattern = re.compile(r\"\\w+|[^\\w\\s]\")  # match words or punctuation\n",
    "\n",
    "    for row in sentimentData.itertuples(index=False):\n",
    "        sentiment = int(row.sentiment)\n",
    "        sentiment_index = (sentiment + 1) // 2\n",
    "        tweet = row.tweet.lower()\n",
    "        tokens = pattern.findall(tweet)\n",
    "\n",
    "        # Negation handling\n",
    "        negated = False\n",
    "        negated_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in negation_words:\n",
    "                negated = True\n",
    "                negated_tokens.append(token)\n",
    "                continue\n",
    "\n",
    "            if token in {\".\", \"!\", \"?\", \";\"}:\n",
    "                negated = False\n",
    "\n",
    "            if negated and re.match(r\"\\w+\", token):  # only apply to word tokens\n",
    "                token = f\"NOT_{token}\"\n",
    "\n",
    "            negated_tokens.append(token)\n",
    "\n",
    "        # Local counters\n",
    "        local_neg = 0\n",
    "        local_pos = 0\n",
    "        local_total = 0\n",
    "\n",
    "        for token in negated_tokens:\n",
    "            stats = word_stats[token]\n",
    "            stats[sentiment_index] += 1\n",
    "            stats[2] += sentiment\n",
    "\n",
    "            if sentiment_index == 0:\n",
    "                local_neg += 1\n",
    "            else:\n",
    "                local_pos += 1\n",
    "            local_total += 1\n",
    "\n",
    "        counts[0] += local_neg\n",
    "        counts[1] += local_pos\n",
    "        counts[2] += local_total\n",
    "    \n",
    "    return word_stats, counts\n",
    "\n",
    "def process_sentiment_data(sentimentData):\n",
    "    \"\"\"\n",
    "    Processes tweet sentiment data to build word sentiment statistics with negation handling.\n",
    "    \n",
    "    Parameters:\n",
    "        sentimentData (pd.DataFrame): DataFrame with columns 'sentiment' and 'tweet'\n",
    "    \n",
    "    Returns:\n",
    "        word_stats (defaultdict): Dictionary mapping word → [neg_count, pos_count, sentiment_sum]\n",
    "        counts (list): [total_neg_tokens, total_pos_tokens, total_tokens]\n",
    "    \"\"\"\n",
    "    word_stats = defaultdict(lambda: [0, 0, 0])  # [neg_count, pos_count, sentiment_sum]\n",
    "    counts = [0, 0, 0]  # total negative, total positive, total tokens\n",
    "    \n",
    "    pattern = re.compile(r\"\\w+|[^\\w\\s]\")  # match words or punctuation\n",
    "\n",
    "    for row in sentimentData.itertuples(index=False):\n",
    "        sentiment = int(row.sentiment)\n",
    "        sentiment_index = (sentiment + 1) // 2\n",
    "        tweet = row.tweet.lower()\n",
    "        tokens = pattern.findall(tweet)\n",
    "\n",
    "        # Negation handling\n",
    "        negated = False\n",
    "        negated_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in negation_words:\n",
    "                negated = True\n",
    "                negated_tokens.append(token)\n",
    "                continue\n",
    "\n",
    "            if token in {\".\", \"!\", \"?\", \";\"}:\n",
    "                negated = False\n",
    "\n",
    "            if negated and re.match(r\"\\w+\", token):  # only apply to word tokens\n",
    "                token = f\"NOT_{token}\"\n",
    "\n",
    "            negated_tokens.append(token)\n",
    "\n",
    "        # Local counters\n",
    "        local_neg = 0\n",
    "        local_pos = 0\n",
    "        local_total = 0\n",
    "\n",
    "        for token in negated_tokens:\n",
    "            stats = word_stats[token]\n",
    "            stats[sentiment_index] += 1\n",
    "            stats[2] += sentiment\n",
    "\n",
    "            if sentiment_index == 0:\n",
    "                local_neg += 1\n",
    "            else:\n",
    "                local_pos += 1\n",
    "            local_total += 1\n",
    "\n",
    "        counts[0] += local_neg\n",
    "        counts[1] += local_pos\n",
    "        counts[2] += local_total\n",
    "    word_stats = {word: stats for word, stats in word_stats.items() if stats[0] + stats[1] >= 5}\n",
    "    return word_stats, counts\n",
    "\n",
    "\n",
    "def process_sentiment_data_withoutstopwords(sentimentData):\n",
    "    \n",
    "    \"\"\"\n",
    "    Processes tweet sentiment data to build word sentiment statistics.\n",
    "    \n",
    "    Parameters:\n",
    "        sentimentData (pd.DataFrame): DataFrame with columns 'sentiment' and 'tweet'\n",
    "    \n",
    "    Returns:\n",
    "        word_stats (defaultdict): Dictionary mapping word → [neg_count, pos_count, sentiment_sum]\n",
    "        counts (list): [total_neg_tokens, total_pos_tokens, total_tokens]\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_stats = defaultdict(lambda: [0, 0, 0])  # [neg_count, pos_count, sentiment_sum]\n",
    "    counts = [0, 0, 0]  # total negative, total positive, total tokens\n",
    "    \n",
    "    pattern = re.compile(r\"\\w+|[^\\w\\s]\")  # match words or punctuation\n",
    "\n",
    "    for row in sentimentData.itertuples(index=False):\n",
    "        sentiment = int(row.sentiment)\n",
    "        sentiment_index = (sentiment + 1) // 2\n",
    "        tweet = row.tweet.lower()\n",
    "        tokens = pattern.findall(tweet)\n",
    "\n",
    "        # Local counters to reduce write contention\n",
    "        local_neg = 0\n",
    "        local_pos = 0\n",
    "        local_total = 0\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in stop_words:\n",
    "                continue\n",
    "            stats = word_stats[token]\n",
    "            stats[sentiment_index] += 1\n",
    "            stats[2] += sentiment\n",
    "\n",
    "            if sentiment_index == 0:\n",
    "                local_neg += 1\n",
    "            else:\n",
    "                local_pos += 1\n",
    "            local_total += 1\n",
    "\n",
    "        counts[0] += local_neg\n",
    "        counts[1] += local_pos\n",
    "        counts[2] += local_total\n",
    "\n",
    "    return word_stats, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766b4c8",
   "metadata": {},
   "source": [
    "The Below code will then take these values stored in our word_stats dictionary, and transform it into a dataframe, allowing us easier lookup of data for calculating the sentiment value of a given tweet using machine prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dc8c9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.S.F or Aggregate Sentiment Frequencies values dataframe, including all words in our tweet library, their counts, and aggregate sentiment values.\n",
    "word_stats_ws, counts_ws = process_sentiment_data_withoutstopwords(sentimentData)\n",
    "word_stats, counts = process_sentiment_data(sentimentData)\n",
    "\n",
    "def toDataFrame(wordStats):\n",
    "    asfValues = pd.DataFrame(\n",
    "        [(word, freq_neg, freq_pos, sentiment_sum) for word, (freq_neg, freq_pos, sentiment_sum) in wordStats.items()],\n",
    "        columns=['Word', 'frequency_negative', 'frequency_positive', 'average_sentiment']\n",
    "    )\n",
    "    # average all sentiment aggregates to become average of the word count and sentiment tracked\n",
    "    asfValues['average_sentiment'] = asfValues['average_sentiment'] / (asfValues['frequency_negative'] + asfValues['frequency_positive'])\n",
    "\n",
    "    #allow lookups by word for easier access to data values\n",
    "    asfValues.set_index('Word', inplace=True)\n",
    "\n",
    "    return asfValues\n",
    "\n",
    "asfValues = toDataFrame(word_stats)\n",
    "asfValues_ws = toDataFrame(word_stats_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5b21a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_stats1, counts1 = process_sentiment_data_1(sentimentData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cd77f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_statsprev, countsprev = process_sentiment_data_prev(sentimentData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d863566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "asfValues_1 = toDataFrame(word_stats1)\n",
    "asfValues_prev = toDataFrame(word_statsprev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71735d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_LLR(word):\n",
    "    if word in asfValues.index:\n",
    "            positive_count = asfValues.loc[word, 'frequency_positive']\n",
    "            negative_count = asfValues.loc[word, 'frequency_negative']        \n",
    "    else:\n",
    "            positive_count = 1  #smoothing constants if word doesn't appear at all in the corpus\n",
    "            negative_count = 1\n",
    "    \n",
    "    alpha = 1  # smoothing constant\n",
    "    V = len(asfValues)  # vocabulary size\n",
    "\n",
    "    P_w_givenGood = (positive_count + alpha)/ (counts[1] + V)\n",
    "    P_w_givenBad = (negative_count + alpha)/ (counts[0] + V)\n",
    "    LLR = np.log(P_w_givenGood/ P_w_givenBad)\n",
    "    biasing_constant = np.log(negative_count + positive_count)\n",
    "    value = biasing_constant * LLR\n",
    "    return value\n",
    "\n",
    "def calculate_word_LLR_prev(word):\n",
    "    if word in asfValues.index:\n",
    "            positive_count = asfValues_prev.loc[word, 'frequency_positive']\n",
    "            negative_count = asfValues_prev.loc[word, 'frequency_negative']        \n",
    "    else:\n",
    "            positive_count = 1  #smoothing constants if word doesn't appear at all in the corpus\n",
    "            negative_count = 1\n",
    "    \n",
    "    alpha = 1  # smoothing constant\n",
    "    V = len(asfValues_prev)  # vocabulary size\n",
    "\n",
    "    P_w_givenGood = (positive_count + alpha)/ (counts[1] + V)\n",
    "    P_w_givenBad = (negative_count + alpha)/ (counts[0] + V)\n",
    "    LLR = np.log(P_w_givenGood/ P_w_givenBad)\n",
    "    biasing_constant = np.log(negative_count + positive_count)\n",
    "    value = biasing_constant * LLR\n",
    "    return value\n",
    "\n",
    "\n",
    "def calculate_word_LLR1(word):\n",
    "    if word in asfValues_1.index:\n",
    "            positive_count = asfValues.loc[word, 'frequency_positive']\n",
    "            negative_count = asfValues.loc[word, 'frequency_negative']        \n",
    "    else:\n",
    "            positive_count = 1  #smoothing constants if word doesn't appear at all in the corpus\n",
    "            negative_count = 1\n",
    "    \n",
    "    alpha = 1  # smoothing constant\n",
    "    V = len(asfValues_1)  # vocabulary size\n",
    "\n",
    "    P_w_givenGood = (positive_count + alpha)/ (counts[1] + V)\n",
    "    P_w_givenBad = (negative_count + alpha)/ (counts[0] + V)\n",
    "    LLR = np.log(P_w_givenGood/ P_w_givenBad)\n",
    "    biasing_constant = np.log(negative_count + positive_count)\n",
    "    value = biasing_constant * LLR\n",
    "    return value\n",
    "\n",
    "def calculate_word_LLR_ws(word):\n",
    "    if word in asfValues.index:\n",
    "            positive_count = asfValues_ws.loc[word, 'frequency_positive']\n",
    "            negative_count = asfValues_ws.loc[word, 'frequency_negative']        \n",
    "    else:\n",
    "            positive_count = 1  #smoothing constants if word doesn't appear at all in the corpus\n",
    "            negative_count = 1\n",
    "    \n",
    "    alpha = 1  # smoothing constant\n",
    "    V = len(asfValues_ws)  # vocabulary size\n",
    "    P_w_givenGood = (positive_count + alpha)/ (counts[1] + V)\n",
    "    P_w_givenBad = (negative_count + alpha)/ (counts[0] + V)\n",
    "    LLR = np.log(P_w_givenGood/ P_w_givenBad)\n",
    "    biasing_constant = np.log(negative_count + positive_count)\n",
    "    value = biasing_constant * LLR\n",
    "    return value\n",
    "\n",
    "def calculate_sentiment_probability_ws(sentence):\n",
    "    pattern = re.compile(r\"\\w+|[^\\w\\s]\")\n",
    "    words = pattern.findall(sentence.lower())\n",
    "    total_llr = 0\n",
    "    for word in words:\n",
    "          stop_words = set(stopwords.words('english'))\n",
    "          if word in stop_words:\n",
    "               continue\n",
    "          LLR = calculate_word_LLR_ws(word)\n",
    "          total_llr += LLR\n",
    "    \n",
    "    return total_llr\n",
    "\n",
    "def calculate_sentiment_probability_1(sentence):\n",
    "    pattern = re.compile(r\"\\w+|[^\\w\\s]\")\n",
    "    words = pattern.findall(sentence.lower())\n",
    "    total_llr = 0\n",
    "    for word in words:\n",
    "          LLR = calculate_word_LLR1(word)\n",
    "          total_llr += LLR\n",
    "    \n",
    "    return total_llr\n",
    "\n",
    "def calculate_sentiment_probability_prev(sentence):\n",
    "    pattern = re.compile(r\"\\w+|[^\\w\\s]\")\n",
    "    words = pattern.findall(sentence.lower())\n",
    "    total_llr = 0\n",
    "    for word in words:\n",
    "          LLR = calculate_word_LLR_prev(word)\n",
    "          total_llr += LLR\n",
    "    \n",
    "    return total_llr\n",
    "\n",
    "def calculate_sentiment_probability(sentence):\n",
    "    pattern = re.compile(r\"\\w+|[^\\w\\s]\")\n",
    "    words = pattern.findall(sentence.lower())\n",
    "    total_llr = 0\n",
    "    for word in words:\n",
    "          LLR = calculate_word_LLR(word)\n",
    "          total_llr += LLR\n",
    "    \n",
    "    return total_llr\n",
    "\n",
    "def evaluate_random_baseline(df):\n",
    "    correct = 0\n",
    "    total = len(df)\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    for row in df.itertuples(index=False):\n",
    "        predicted_sentiment = random.choice([-1, 1])  # Random guess\n",
    "        actual_sentiment = int(row.sentiment)\n",
    "\n",
    "        predictions.append(predicted_sentiment)\n",
    "        actuals.append(actual_sentiment)\n",
    "\n",
    "        if predicted_sentiment == actual_sentiment:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Random Baseline Accuracy: {accuracy:.2%}\")\n",
    "    return predictions, actuals\n",
    "\n",
    "def evaluate_llr_model_ws(df):\n",
    "    correct = 0\n",
    "    total = len(df)\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    for row in df.itertuples(index=False):\n",
    "        llr_score = calculate_sentiment_probability_ws(row.tweet)\n",
    "        predicted_sentiment = 1 if llr_score > 0 else -1  # LLR > 0 → positive, else negative\n",
    "        actual_sentiment = int(row.sentiment)\n",
    "\n",
    "        predictions.append(predicted_sentiment)\n",
    "        actuals.append(actual_sentiment)\n",
    "\n",
    "        if predicted_sentiment == actual_sentiment:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Excluding Stopword Model Predicted Accuracy: {accuracy:.2%}\")\n",
    "    return predictions, actuals\n",
    "\n",
    "def evaluate_llr_model_prev(df):\n",
    "    correct = 0\n",
    "    total = len(df)\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    for row in df.itertuples(index=False):\n",
    "        llr_score = calculate_sentiment_probability_prev(row.tweet)\n",
    "        predicted_sentiment = 1 if llr_score > 0 else -1  # LLR > 0 → positive, else negative\n",
    "        actual_sentiment = int(row.sentiment)\n",
    "\n",
    "        predictions.append(predicted_sentiment)\n",
    "        actuals.append(actual_sentiment)\n",
    "\n",
    "        if predicted_sentiment == actual_sentiment:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Milestone 3 Model Accuracy: {accuracy:.2%}\")\n",
    "    return predictions, actuals\n",
    "\n",
    "def evaluate_llr_model_1(df):\n",
    "    correct = 0\n",
    "    total = len(df)\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    for row in df.itertuples(index=False):\n",
    "        llr_score = calculate_sentiment_probability(row.tweet)\n",
    "        predicted_sentiment = 1 if llr_score > 0 else -1  # LLR > 0 → positive, else negative\n",
    "        actual_sentiment = int(row.sentiment)\n",
    "\n",
    "        predictions.append(predicted_sentiment)\n",
    "        actuals.append(actual_sentiment)\n",
    "\n",
    "        if predicted_sentiment == actual_sentiment:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Normal Model Predicted Accuracy with Negative Weighting: {accuracy:.2%}\")\n",
    "    return predictions, actuals\n",
    "\n",
    "def evaluate_llr_model(df):\n",
    "    correct = 0\n",
    "    total = len(df)\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    for row in df.itertuples(index=False):\n",
    "        llr_score = calculate_sentiment_probability(row.tweet)\n",
    "        predicted_sentiment = 1 if llr_score > 0 else -1  # LLR > 0 → positive, else negative\n",
    "        actual_sentiment = int(row.sentiment)\n",
    "\n",
    "        predictions.append(predicted_sentiment)\n",
    "        actuals.append(actual_sentiment)\n",
    "\n",
    "        if predicted_sentiment == actual_sentiment:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Normal Model Predicted Accuracy with Negative Weighting, and removal of least frequent words: {accuracy:.2%}\")\n",
    "    return predictions, actuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f67e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milestone 3 Model Accuracy: 75.57%\n",
      "Normal Model Predicted Accuracy with Negative Weighting, and removal of least frequent words: 76.52%\n",
      "Normal Model Predicted Accuracy with Negative Weighting: 76.52%\n"
     ]
    }
   ],
   "source": [
    "# test the data against tweets in the dataset\n",
    "negative_tweets = sentimentData[sentimentData['sentiment'] == -1].sample(n=5000, random_state=6)\n",
    "positive_tweets = sentimentData[sentimentData['sentiment'] == 1].sample(n=5000, random_state=6)\n",
    "balanced_sample = pd.concat([negative_tweets, positive_tweets]).reset_index(drop=True)\n",
    "\n",
    "evaluate_llr_model_prev(balanced_sample)\n",
    "evaluate_llr_model(balanced_sample)\n",
    "evaluate_llr_model_1(balanced_sample)\n",
    "evaluate_llr_model_ws(balanced_sample)\n",
    "evaluate_random_baseline(balanced_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
